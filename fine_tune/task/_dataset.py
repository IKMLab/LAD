r"""Fine-tune task' dataset and its utilities.

`fine_tune.task.Dataset` must never used to construct instance. All fine-tune
task's dataset classes must inherit from `fine_tune.task.Dataset`.

Usage:
    import fine_tune

    class CustomizedDataset(fine_tune.task.Dataset):
        ...

    num_label = fine_tune.task.get_num_label(CustomizedDataset)

    assert fine_tune.task.label_encoder(
        CustomizedDataset,
        CustomizedDataset.allow_labels[0]
    ) == 0

    assert fine_tune.task.label_decoder(
        CustomizedDataset,
        0
    ) == CustomizedDataset.allow_labels[0]
"""

# built-in modules

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import abc
import logging

from typing import Callable
from typing import List
from typing import Optional
from typing import Tuple
from typing import Type
from typing import TypedDict
from typing import Union

# 3rd party modules

import torch
import torch.utils
import torch.utils.data

# Define types for type annotation.

Label = Union[bool, int, str]


class Sample(TypedDict):
    r"""Sample data structure.

    We structured each sample data as a `dict` which is compatible with method
    `transformers.PreTrainedModel.forward`. `text_pair` is only used when input
    text consist of 2 sequences. `text_pair` must be `None` if input text
    consist of only 1 sequence.
    See https://huggingface.co/transformers/main_classes/model.html for
    `transformers.PreTrainedModel.forward` method signature.
    """
    index: int
    text: str
    text_pair: Optional[str]
    #TODO: which value can represent testing sample? (current: we use `-1`)
    label: int


# `collate_fn` input list(Sample) and return
# `tuple(input_id, attention_mask, token_type_ids, label, logits)`.
# Each field in the returned tuple must have following numeric type:
# - `input_id.dtype == torch.int64`
# - `attention_mask.dtype == torch.float32`
# - `token_type_ids.dtype == torch.int64`
# - `label.dtype == torch.int64`

CollateFnReturn = Tuple[
    List[str],
    Optional[List[str]],
    List[int],
]

CollateFn = Callable[
    [List[Sample]],
    CollateFnReturn
]

ContrastCollateFnReturn = Tuple[
    List[str], # text
    Optional[List[str]], # text pair
    List[int], # label
    List[int], # index of positive sample
    List[List[int]] # indices of negative samples
]

ContrastCollateFn = Callable[
    [List[Tuple[Sample, int, List[int]]]],
    ContrastCollateFnReturn
]

# Get logger.

logger = logging.getLogger('fine_tune.task')

# Define dataset base class.


class Dataset(torch.utils.data.Dataset):
    r"""Fine-tune task's dataset and its utilities.

    When we say "task" we mean a more general dataset waited to be solve. When
    we say "dataset" we mean a particular dataset provide by the "task". (e.g.,
    some task might have both training and develop dataset and some are not.)

    Args:
        dataset:
            Name of the datset file to be loaded. When `dataset` is the name of
            some previous experiment, it means the logits dataset generated by
            the model of that experiment.

    Attributes:
        allow_dataset:
            Allowed dataset in the task. All dataset which are not generated by
            some previous experiment must go in to `allow_dataset`.
        allow_labels:
            Allowed labels in the task.
        dataset:
            A list of samples of the dataset.
        task_path:
            Path of the task contains all dataset.
    """
    allow_dataset: List[str] = []

    allow_labels: List[Label] = []

    task_path: str = ''

    def __init__(self, dataset: str):
        # Load task specific dataset.
        if dataset in self.__class__.allow_dataset:
            logger.info(
                'Start loading task %s dataset %s.',
                self.__class__.__name__,
                dataset
            )
            self.dataset = self.__class__.load(dataset)
            logger.info(
                'Finish loading task %s dataset %s.',
                self.__class__.__name__,
                dataset
            )
        else:
            raise ValueError(f"This dataset: {dataset} is not allowed!")

    def __getitem__(self, index: int) -> Sample:
        r"""Sample dataset by index.

        This method is required by `torch.utils.data.Dataset` and
        `torch.utils.data.DataLoader`. Since `self.dataset` is a list, we will
        return `self.dataset[index]` directly.

        Args:
            index:
                Sample index of dataset.

        Raises:
            IndexError:
                `self.dataset` index out of range.

        Returns:
            Sample of that index.
        """
        return self.dataset[index]

    def __len__(self) -> int:
        r"""Return dataset size.

        This method is required by `torch.utils.data.Dataset` and
        `torch.utils.data.DataLoader`. Since `self.dataset` is a list, we will
        return `len(self.dataset)` directly.

        Returns:
            The length of the list `self.dataset`.
        """
        return len(self.dataset)

    @staticmethod
    @abc.abstractmethod
    def load(dataset: str) -> List[Sample]:
        r"""Load dataset into memory.

        This is a heavy IO method and might required lots of memory since
        dataset might be huge. All task dataset must be download previously.
        See task document in 'project_root/doc/' for downloading details.

        Args:
            dataset:
                Name of the dataset to be loaded.

        Raises:
            FileNotFoundError:
                When dataset file does not exist.

        Returns:
            A list of samples of the dataset.
        """
        raise NotImplementedError(
            'Missing static method `load`.'
        )


    @staticmethod
    def create_collate_fn() -> CollateFn:
        r"""Create `collate_fn` used by `torch.utils.data.Dataloader`.

        Returns:
            `collate_fn` function used by `torch.utils.data.Dataloader`.
        """
        def collate_fn(batch_samples: List[Sample]) -> CollateFnReturn:
            text = []
            label = []
            text_pair = []

            # When input text is consist of only 1 sequence.
            if batch_samples[0]['text_pair'] is None:
                text_pair = None
                for sample in batch_samples:
                    text.append(sample['text'])
                    label.append(sample['label'])
            # When input text is consist of 2 sequences.
            else:
                for sample in batch_samples:
                    text.append(sample['text'])
                    text_pair.append(sample['text_pair'])
                    label.append(sample['label'])

            return (
                text,
                text_pair,
                label
            )

        return collate_fn

# Define dataset utility.

def label_encoder(
        cls: Type[Dataset],
        label: Label
) -> int:
    r"""Encode label into number.

    Args:
        cls:
            `fine_tune.task.Dataset` subclass which has class attribute
            `allow_labels`.
        label:
            `label` is encoded as its respective index of `allow_labels`.

    Raises:
        ValueError:
            When `label` is not in values of `cls.allow_labels`.

    Returns:
        Encoded label id.
    """
    try:
        return cls.allow_labels.index(label)
    except ValueError:
        raise ValueError(
            f'unexpected {cls.__name__} label: {label}'
        )

def label_decoder(
        cls: Type[Dataset],
        label_id: int
) -> Label:
    r"""Decode number into label.

    Args:
        cls:
            `fine_tune.task.Dataset` subclass which has class attribute
            `allow_labels`.
        label_id:
            `label_id` is decoded as its respective value of `allow_labels`.

    Raises:
        ValueError:
            When `label_id` is not in indice of `cls.allow_labels`.

    Returns:
        Decoded label.
    """
    try:
        # Avoid negative `label_id` like `label_id == -1` in order to abstract
        # `cls.allow_labels` from user.
        if label_id < 0:
            raise ValueError

        return cls.allow_labels[label_id]
    except (IndexError, ValueError):
        raise ValueError(
            f'unexpected {cls.__name__} label id: {label_id}'
        )


def get_num_class(cls: Type[Dataset]) -> int:
    r"""Return number of classes to classify.

    Args:
        cls:
            `fine_tune.task.Dataset` subclass which has class attribute
            `allow_labels`.

    Returns:
        Total number of different classes.
    """
    return len(cls.allow_labels)
